# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/131iwrl4duztsVCIX3sUx7FqGrbkCPjRO
"""

# Homework1
import torch
import torch.nn as nn

data = torch.tensor([1, 2, 3], dtype=torch.float) # Change the data type to float

Softmax_function = nn.Softmax(dim=0)
output = Softmax_function(data)
output

class Mysoftmax(nn.Module): #Softmax
  def __init__(self):
    super().__init__()
  def forward(self,x):
    torch.exp(x)
    x_exp = torch.exp(x)
    total = x_exp.sum(0, keepdim = True)
    return x_exp/total
my_softmax = Mysoftmax()
output = my_softmax(data)
output

class Mystablesoftmax(nn.Module):   # Stable softmax
  def __init__(self):
    super().__init__()
  def forward(self,x):
    torch.max(data, dim=0, keepdim=True).values
    c = torch.max(x,dim=0)
    torch.exp(x)
    x_exp = torch.exp(x - c.values)
    total = x_exp.sum(0, keepdim=True)
    return x_exp /total
mystablesoftmax = Mystablesoftmax()
output = mystablesoftmax(data)
output

